# Gradient Boosting Models


## C.  Entrenamiento del modelo GBM-clasificación

```{r}
if (!require(gbm)) install.packages('gbm')
library(gbm)
```

Creamos un objeto de clase 'formula' y se lo pasamos como argumento a la función `gbm`
```{r}
xVars <- colnames(AutoBi[ , -match(c("SEATBELT","CASENUM","LOSS"), colnames(AutoBi)) ])
targets <- c("LOSS","Y")

fmla.gbm1 <- as.formula(paste0(targets[2],"~",paste0(xVars,collapse = "+"),collapse = ""))

set.seed(999)

gbm1 <- gbm(fmla.gbm1,
            data = dt_train,
            distribution="bernoulli",
            n.trees = 5000,
            interaction.depth = 3,
            n.minobsinnode = 20,
            shrinkage = 0.001 )

summary(gbm1)
```

Como valor estimado se obtiene la probabilidad condicional  $ P(Y=1|X_1=ATTORNEY,\ldots,X_6=SEATBELT)$

Miramos el resultado en el test:
  
```{r}
gbm1.prediction <- as.data.frame(predict.gbm(gbm1, newdata = dt_train , n.trees = 5000 , type="response"))

dt_train$pred_gbm1 <- gbm1.prediction[[1]]
```


Calculamos la medida de "performance" del mecanismo (modelo) de predicción 'gbm1'
```{r}
gbm1.pred <- prediction(dt_train$pred_gbm1,dt_train$Y) # con el train creamos un

gbm1.perf <- performance(gbm1.pred,"tpr","fpr") 
```  

Calculamos el punto de corte
```{r}
gbm1.perf@alpha.values[[1]][gbm1.perf@alpha.values[[1]]==Inf] <- round(max(gbm1.perf@alpha.values[[1]][gbm1.perf@alpha.values[[1]]!=Inf]),2)

KS.matrix= cbind(abs(gbm1.perf@y.values[[1]]-gbm1.perf@x.values[[1]]), gbm1.perf@alpha.values[[1]])

colnames(KS.matrix) <- c("KS-distance","cut-point")
head(KS.matrix)

ind.ks  <- sort( KS.matrix[,1] , index.return=TRUE )$ix[nrow(KS.matrix)]
```

El punto de corte óptimo de KS
```{r}
gbm1.KScutoff <- KS.matrix[ind.ks,2]
```

(Opcional) Area bajo la curva ROC
```{r}
gbm1.auc <- performance(gbm1.pred ,"auc")@y.values[[1]]
```  

(opcional) Curva ROC GBM - train
```{r}
#win.graph()
plot( gbm1.perf , col='red' , lwd=2, type="l" , main="Curva ROC modelo GBM",xlab="Tasa de falsos positivos", ylab="Tasa de verdaderos positivos")
abline( 0 , 1 , col="blue" , lwd=2, lty=2)
abline( 0 , 0 , 1 , col="gray40"   , lty=3)
legend( 0.4  , 0.2 , c(paste0("AUC (Gradient Boosting)=",round(gbm1.auc,4)),"AUC (Coin toss)=0.50") ,lty=c(1,2), lwd=c(2,2) ,col=c("red","blue"), bty="n")
```  


Calculamos la predicción en el test y evaluamos el poder de clasificación del modelo
```{r}  
gbm1.pred_test    <- as.data.frame(predict.gbm( gbm1, newdata = dt_test , n.trees = 5000 , type = "response"))
dt_test$pred_gbm1 <- gbm1.pred_test[[1]] 
```  

Con el test creamos un objeto de tipo 'prediction'
```{r}
dt_gbm1.pred  <- prediction(dt_test$pred_gbm1,dt_test$Y) 
dt_gbm1.perf  <- performance(dt_gbm1.pred,"tpr","fpr") 
```  

Evaluación del poder de clasificaci?n del modelo GBM1 vía curva ROC
```{r}  
gbm1.auc_test <- performance( dt_gbm1.pred ,"auc")@y.values[[1]]
``` 

 - Error tipo I ($\alpha$):       10.87% indica el error que se comete clasificando una pérdida 'severa' como 'no severa'
 - Error tipo II ($\beta$):       39.89% indica el error que se comete clasificando una pérdida 'no severa' como 'severa'
 - % de mala clasificación:    35.81% nos da el % de veces que el modelo clasifica incorrectamente las pérdidas 
 - Accuracy = $100 - %mc$:        63.43% nos da el % de veces que el modelo acierta clasificando las pérdidas
 - Area bajo la curva ROC, $AUC$: 0.7583 nos da una medida global del poder de clasificación del modelo
 - Finalmente calculamos la curva ROC junto con la métrica AUC 

Evaluamos el poder de clasificación con la función `metricBinaryClass`:
```{r}  
#metricBinaryClass( fitted.model = gbm1 , dataset= dt_test , cutpoint=gbm1.KScutoff , roc.graph=TRUE)
```



## D.  Entrenamiento del modelo GBM-regresión

```{r}  
fmla.gbm2 <- as.formula(paste0(targets[1],"~",paste0(xVars,collapse = "+"),collapse = ""))

set.seed(1234)

gbm2 <- gbm(fmla.gbm2,
            data = dt_train,
            distribution="gaussian",
            n.trees = 5000,
            interaction.depth = 3,
            n.minobsinnode = 20,
            #keep.data = TRUE,
            #n.cores = 4,
            shrinkage = 0.001 )

summary(gbm2)
```

```{r} 
str(gbm2)
```

Como valor estimado se obtiene la esperanza condicional de la pérdida: $E(LOSS|X_1=ATTORNEY,...,X_6=SEATBELT)$

Miramos el resultado en el test:
```{r}   
gbm2.prediction <- as.data.frame(predict.gbm(gbm2, newdata = dt_test , n.trees = 5000 , type="response"))
dt_test$pred_gbm2 <- gbm2.prediction[[1]]
head(dt_test)
``` 

```{r} 
tail(dt_test)
```

```{r} 
summary(dt_test)
```


Graficamos la distribución de los valores estimados en el train
```{r} 
plot(density(dt_test$pred_gbm2[!is.na(dt_test$pred_gbm2) & dt_test$pred_gbm2 < 30]), ylim= c(0,.25) , xlab="Predicted", col="red" , main="")
lines(density(dt_test$LOSS[dt_test$LOSS<30]),col="blue",lty=1)
lines(density(dt_test$pred_rf2[!is.na(dt_test$pred_rf2) & dt_test$pred_rf2 < 30]),col="darkgreen",lty=1)
```

```{r} 
modelchecktest2 <- as.data.frame( cbind(real=dt_test$LOSS , predicted=dt_test$pred_gbm2) )
modelchecktest2[is.na(modelchecktest2)] <- 0
summary(modelchecktest2)
```

Error de ajuste del modelo

```{r}   
plot(modelchecktest2, xlim=c(0,100) , ylim=c(0,100) ,  pch="." , cex=1.5)
segments( 0, 0 , 100, 100 , col="red")

#modelMetrics(real=modelchecktest2$real, pred=modelchecktest2$predicted )
```


Comentario: Al igual que en el caso del modelo de regresión RF2, el error de ajuste del modelo GBM2 demasiado alto: $RMSE= 53.82k$ y el $MAPE=134.19%$. Con estos errores, es preferible ir a un modelo de clasificación en lugar de un modelo de regresión.


# Support Vector Machines

## E.  Entrenamiento del modelo SVM-clasificación

```{r} 
if (!require(e1071)) install.packages('e1071')
library(e1071)
```

librería alternativa que también es muy utilizada
```{r} 
if (!require(kernlab)) install.packages('kernlab')
library(kernlab) 
```

Creamos la relación (fórmula) entre las variables

```{r} 
fmla.svm <-  as.formula(paste0(targets[2],"~",paste0( c(xVars,"SEATBELT"),collapse = "+"),collapse = ""))
```

Modelo exploratorio (sin ajustar parámetros):
  ```{r} 
svm0 <- svm( fmla.svm , data=dt_train , type="C-classification", cost=10, na.action = na.omit)
```

Utilizamos la función `na.index` para recuperar las posiciones de todos los registros donde hay missings (NA's, NULL's, blanks)
```{r} 
#ids_NAs <- na.index(dt_test) 
```

Es necesario eliminar los valores missings (NA) de la variable a modelizar (target). Las posiciones están almacenadas en la variable 'ids_NAs'
```{r} 
#real_target <- data.frame( real = dt_test[ -ids_NAs , 'Y']) 
#predicted_target <- data.frame(predicted = predict(svm0, dt_test))
#pred.svm0 <- cbind( real_target , predicted_target )

#metricBinaryClass( fitted.model = NULL , dataset= pred.svm0 , cutpoint= NULL , roc.graph=FALSE)
```

Este modelo ofrece un pobrísimo poder de clasificación. Hay que buscar un mejor modelo vía 'tuning' de los parámetros. A continuación vemos cómo realizar esto:
  
Comparamos dos funciones kernel:  'Base radial' vs 'sigmoide' (no debemos utilizar el kernel 'lineal') 

```{r} 
t.ini <- Sys.time()
tuneSVMclass <- tune(svm, fmla.svm,  data = dt_train, ranges = list( kernel='sigmoid', gamma= seq(1,3,by=.5) ,                                              cost = seq(5,100,by=5)) ,
                     tune.control = tune.control( random = TRUE, sampling = "bootstrap", sampling.aggregate = mean, 
                                                  nboot = 10, boot.size = 0.90, best.model = TRUE, performances = TRUE, error.fun = 'auc'))                    

t.end <- Sys.time()
print(t.end-t.ini)
print(tuneSVMclass)
```


El mejor modelo de clasificación vía SVM
```{r} 
tunedSVM1 <- svm( fmla.svm , data=dt_train, type="C-classification", kernel='sigmoid' , probability = TRUE , gamma= 1, cost =10 , na.action = na.omit)
```

El mejor SVM se obtiene con un **kernel de base radial**
```{r}  
predicted_target <- as.data.frame(predict( tunedSVM1 , dt_test))
#pred.svm1        <- as.data.frame(cbind( real_target ,  predicted_target ))
#colnames(pred.svm1) <- c('actual','predicted')

#table( pred.svm1 )
```

Curva ROC SVM

```{r} 
#metricBinaryClass( fitted.model = NULL , dataset=  pred.svm1   , cutpoint= NULL , roc.graph=TRUE)  

#metricBinaryClass( fitted.model = tunedSVM1 , dataset=  dt_test  , cutpoint= NULL , roc.graph=TRUE)  
``` 


Evaluamos el poder de clasificación con la función 'metricBinaryClass':
  ```{r}   
#metricBinaryClass( fitted.model = NULL , dataset= pred.svm.model , cutpoint= NULL , roc.graph=FALSE)
```

 - Error tipo I ($\alpha$) : 79.07% nos dice el error que se comete clasificando una pérdida 'severa' como 'no severa'
 - Error tipo II ($\beta$) : 10.73% nos dice el error que se comete clasificando una p?rdida 'no severa' como 'severa'
 - % de mala clasificación : 19.58% nos da el % de veces que el modelo clasifica incorrectamente las pérdidas 
 - Accuracy = $100 -%mc$   : 80.42%
 - Area bajo la curva ROC, $AUC$ : 0.55102
 - Finalmente calculamos la curva ROC junto con la métrica AUC 



## F.  Entrenamiento del modelo SVM-regresión


Para entrenar el SVM - regresión vamos directamente a buscar el mojor modelo vía 'tuning' de los parámetros:

  - Usando una función Kernel de base radial
```{r}
#t.ini <- Sys.time()
#tuneSVMreg <- tune(svm, fmla.svm,  data = dt_train, ranges = list(epsilon = seq(0,1,by=0.1), gamma= seq(0.1,3,by=0.1) , cost = seq(5,100, by=5) ), tune.control= tune.control(random = TRUE, nrepeat = 10, repeat.aggregate = mean, sampling = "bootstrap", sampling.aggregate = mean, samling.dispersion = sd, cross = 10, fix = 2/3, nboot = 10, boot.size = 0.90, best.model = TRUE, performances = TRUE, error.fun = 'mse'))
#t.end <- Sys.time()
#print(t.end-t.ini)
```

El mejor modelo SVM

# Incompleto...



###                                    G.  comparativa de modelos de clasificaci?n

Realizamos una breve comparativa de los distintos m?todos de clasificaci?n de la p?rdidas:
  
- Error tipo I (alpha)       : 79.07% nos dice el error que se comete clasificando una p?rdida 'severa' como 'no severa'
- Error tipo II (beta)       : 10.73% nos dice el error que se comete clasificando una p?rdida 'no severa' como 'severa'
- % de mala clasificaci?n    : 19.58% nos da el % de veces que el modelo clasifica incorrectamente las p?rdidas 
- Accuracy = 100 -%mc        : 80.42%
  - Area bajo la curva ROC auc : 0.55102
- Finalmente calculamos la curva ROC junto con la m?trica AUC 


### Curvas ROC
```{r}
#win.graph()
plot( gbm1.perf , col='red' , lwd=2, type="l" , main="Curva ROC modelo GBM",xlab="Tasa de falsos positivos", ylab="Tasa de verdaderos positivos")
abline( 0 , 1 , col="blue" , lwd=2, lty=2)
abline( 0 , 0 , 1 , col="gray40"   , lty=3)
legend( 0.4  , 0.2 , c(paste0("AUC (Gradient Boosting)=",round(gbm1.auc,4)),"AUC (Coin toss)=0.50") ,lty=c(1,2), lwd=c(2,2) ,col=c("red","blue"), bty="n")
```

### falta - Incompleto
