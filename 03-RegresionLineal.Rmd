# (PART) Modelos Estadísticos {-} 

```{r setup, include=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)      # For grabbing the dimensions of png files
knitr::opts_chunk$set(echo = TRUE)
imgs_path = "imgs/"
```

# Regresión Lineal

```{r echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, comment=FALSE}
y= c(122,118,128,121,125,136,144,142,149,161,167,168,162,171,175,182,180,183,188,200,194,206,207,210,219)
x=c(50,53,54,55,56,59,62,65,67,71,72,74,75,76,79,80,82,85,87,90,93,94,95,97,100)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, comment=FALSE}
plot(x,y,cex=1.2)
```


```{r echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, comment=FALSE}
y= c(122,118,128,121,125,136,144,142,149,161,167,168,162,171,175,182,180,183,188,200,194,206,207,210,219)
x=c(50,53,54,55,56,59,62,65,67,71,72,74,75,76,79,80,82,85,87,90,93,94,95,97,100)
regresion=lm(y~x)
plot(x,y,cex=1.2)
abline(regresion, col='blue', lwd=1,lty=3)
```




<!-- <img src='http://reliawiki.org/images/8/81/Doe4.4.png' alt="Linear Regression Model" style="float:width:90%;"> -->


```{r  out.width = "90%", echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, comment=FALSE}
img_path =paste(imgs_path,"Doe4.4.png",sep="")
include_graphics(img_path) 
```



<!-- <img src='http://reliawiki.org/images/2/28/Doe4.3.png' alt="Linear Regression Model" style="float:width:90%;"> -->



```{r  out.width = "90%", echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, comment=FALSE}
img_path =paste(imgs_path,"Doe4.3.png",sep="")
include_graphics(img_path) 
```












<br>

<div class="rmdcomment">

Nota sobre **Modelo Estadístico**

 - Los datos varian según las condiciones de su contexto experimental.  

 - La **variabilidad** en los datos, puede ser  expresada de manera simplificada a través de un modelo, conformado por una **ecuación** y una serie de **hipótesis** sobre las componentes de azar que subyacen el estudio.

 - La **ecuación** del modelo incluye siempre dos partes, una **determinísta** asociada con variaciones sistemáticas y que se reconoce que van a existir incluso antes de realizar el experimento  y otra que depende de **componentes aleatorias** que son imposible de controlar y usualmente inherentes a la variabilidad propia del fenómeno aleatorio en estudio.

    $$ \text{Y} = \text{Funcion Deterministica} + \text{Perturbacion Aleatoria} $$

 - También se puede decir que en un modelo estadístico hay siempre dos estructuras íntimamente relacionadas:

    - La **estructura de media** (que provee el valor esperado para la respuesta bajo las condiciones experimentales)
    - La **estructura de varianzas y covarianzas** (asociada a la o las componentes aleatorias del modelo).




## ¿Qué es un Modelo de Regresión?

<br>
<div class="info">
 El modelo de regresión se utiliza para representar la relación entre $Y$ y $X$: 
 $$Y = f(x)$$
</div>


  - $Y$ es una variable respuesta, explicada o dependiente: que depende de otras y que tratamos de explicar/predecir.
  - $X$, o mas bien $X_1, X_2, \ldots, X_K$ son variables explicativas o independientes que permiten explicar/predecir $Y$.



<br>
<div class="info">
 Una regresión es lineal, cuando la función $f(x)$ que relaciona $X$ e $Y$ es una **función lineal**.
</div>

Teniendo $K$ variables explicativas, la regresión lineal es:

$$ Y = \beta_0  + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_K X_K + \epsilon; \epsilon \sim N(0,\sigma^2) $$

De manera que el valor de la predicción de la variable $Y$ será:

$$ \hat{Y}  = \beta_0  + \beta_1 X_1 + \beta X_2 + ... + \beta X_K $$

O lo que es lo mismo:

$$  \epsilon = Y - \hat{Y} $$
Así, $\epsilon$  será el error cometido en la previsión de $Y$ usando el mode.


<br>
<div class="info">
En función del número $K$ de variables explicativas que tengamos, la regresión lineal puede ser **Simple** o **Múltiple**.
</div>

  - **Simple** si hay una única variable independiente ($K = 1$).
  - **Múltiple** si hay varias variables independendientes ($K > 1$).


Para obtener la $\hat{Y}$ es necesario conocer $\beta_0, \ldots, \beta_K$, es decir, falta el proceso de **inferencia estadística**.




<br>
<div class="info">
 La **Inferencia Estadística** es el procedimiento que permiten elaborar conclusiones sobre parámetros poblacionales desconocidos. 
</div>



$$ \hat{Y}  = \hat{\beta}_0  + \hat{\beta}_1 X_1 + \hat{\beta} X_2 + ... + \hat{\beta} X_K $$


  - Conocer o estimar a un parámetro de la distribución de una variable es posible a través de un estadístico (estadístico muestral). 
  - Dado que el estadístico es obtenido a partir de una muestra y hay más de una muestra posible de ser elegida, el valor del estadístico dependerá de la muestra seleccionada. 
  - Como los valores de los estadísticos cambian de una muestra a otra. Interesa contar con una medida de estos cambios para cuantificar la medida del error en el que podría incurrirse al hacer una inferencia.



<br>
<div class="rmdcomment">

 Los parámetros de un modelo de regresión se pueden estimar con:
 
 - **Mínimos Cuadrados Ordinarios (OLS)**
 - **Máxima Verosimilitud (ML)**
 - **Inferencia Bayesiana**
 
</div>



### Ver también:

 - [Simple Linear Regression Analysis](http://reliawiki.org/index.php/Simple_Linear_Regression_Analysis)
 
  - [¿Cómo Validar Tu Modelo De Regresión?](https://www.maximaformacion.es/master-estadistica-aplicada-con-r/blog/item/como-validar-tu-modelo-de-regresion.html)


 - [¿Qué modelo de regresión debería elegir?]( https://www.maximaformacion.es/master-estadistica-aplicada-con-r/blog/item/que-modelo-de-regresion-deberia-elegir.html)
 

 - [¿Cómo seleccionar las variables adecuadas para tu modelo?](https://www.maximaformacion.es/master-estadistica-aplicada-con-r/blog/item/como-seleccionar-las-variables-adecuadas-para-tu-modelo.html)


-[Multiple Linear Regression Analysis](http://reliawiki.org/index.php/Multiple_Linear_Regression_Analysis)


## Práctica en R

### Regresión lineal simple



Para realizarla usaremos la base de datos `Boston` de la librería `MASS`. 
```{r}
library(MASS)
```

Veamos la descripción de `Boston` en la ayuda de R:

```{r}
?Boston
```

Observamos que es un data.frame con 506 observaciones y 14 variables. 

Podemos explorar un poco más los datos usando las funciones `head`, `tail` y `summary`.

```{r}
head(Boston,5)
```

```{r}
tail(Boston,5)
```

```{r}
summary(Boston)
```

Antes de continuar, hacemos la división de `Boston` en `train`y `test`.
```{r}
id_train <- sample(1:nrow(Boston), size = 0.8*nrow(Boston))
train <- Boston[id_train, ]
test <- Boston[-id_train, ]
```

Ajustamos el modelo de regresión lineal simple para predecir la variable `medv` utilizando la variable `lstat` de nuestro conjunto de datos `Boston`. Para ello usaremos la función `lm`.
```{r}
reg_ls <- lm(medv~lstat, data = train)
reg_ls
```

Veamos los coeficientes de la regresión
```{r}
reg_ls$coefficients
```

Donde el término independiente es:
```{r}
reg_ls$coefficients[1]
```

y el coeficiente de la variable `lstat`es:
```{r}
reg_ls$coefficients[2]
```

De manera que la recta de regresión lineal, siendo $y$ la variable `medv` y $x$ la variable `lstat`,  es:
```{r, echo=FALSE}
cat('y = ', reg_ls$coefficients[1],'+ ', reg_ls$coefficients[2],'x')
```


Si queremos obtener los errores residuales de las observaciones correspondientes:
```{r}
residuales <- reg_ls$residuals
# Veamos los residuales de las 10 primeras observaciones
residuales[1:10]
```

Una vez obtenido el modelo de regresión lineal, para realizar la predicción sobre un nuevo conjunto de datos, utilizamos la función `predict`, de la siguiente manera:
```{r}
predic <- predict(reg_ls, newdata = test)
#Veamos la predicción de las 10 primeras observaciones
predic[1:10]
```

Algunas representaciones gráficas de un modelo de regresión son:

 - Dispersión de los puntos y la recta de regresión lineal simple obtenida:
```{r}
regresion <- lm(medv~lstat, data = Boston)

plot(Boston$lstat, Boston$medv, xlab = "lstat", ylab = "medv")
abline(regresion, col='red', lwd=2)
a <- regresion$coefficients[[1]]
b <- regresion$coefficients[[2]] 
text(30,40,labels = paste('Y = ', round(b,2),'x +', round(a,2)), col='red')
```

 - Análisis de residuos:
```{r}
par(mfrow=c(2,2))
plot(regresion)
```



### Regresión lineal múltiple
Utilizamos lo mismo que hemos hecho para la regresión lineal simple, con la diferencia de que ahora hay más de una variable independiente.
Usamos la misma función, `lm`, y la sucesión de variables independientes estarán separadas con un `+`, es decir:
```{r}
reg_lm <- lm(medv~lstat + age, data = train)
reg_lm
```
Veamos los coeficientes de la regresión
```{r}
reg_lm$coefficients
```

Donde el término independiente es:
```{r}
reg_lm$coefficients[1]
```

el coeficiente de la variable `lstat` es:
```{r}
reg_lm$coefficients[2]
```
y el coeficiente de la variable `age` es:
```{r}
reg_lm$coefficients[3]
```

De manera que la recta de regresión lineal, siendo $y$ la variable `medv`, $x1$ la variable `lstat` y $x2$ la variable `age`, será:
```{r, echo=FALSE}
cat('y = ', reg_lm$coefficients[1],'+ ', reg_lm$coefficients[2],'x1 +', reg_lm$coefficients[3],'x2' )
```

Veamos los gráficos de dispersión 2 a 2:
```{r]}
pairs(Boston[,c('medv','lstat', 'age')],panel = panel.smooth)
```

Análogamente a como hemos hecho con la regresión lineal, podemos obtener los residuos y utilizar la función `predict` en un nuevo conjunto de datos.

Hay otras opciones de poner la variables independientes. Por ejemplo, si quisieramos usar todas las variables, como conjunto de variables independientes, bastaría con escribir:
```{r}
reg_lm2 <- lm(medv~., data = train)
reg_lm2
```

Por otro lado, si quisieramos usarlas todas excepto alguna, podemos escribir:
```{r}
reg_lm3 <- lm(medv~.-age, data = train)
reg_lm3
```







